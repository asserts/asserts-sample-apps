# Default values for mysql-app.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: ai.asserts.mysql-app
  pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: "mysql-app"
fullnameOverride: "mysql-app"

#serviceAccount:
#  # Specifies whether a service account should be created
#  create: true
#  # Annotations to add to the service account
#  annotations: {}
#  # The name of the service account to use.
#  # If not set and create is true, a name is generated using the fullname template
#  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000


service:
  type: ClusterIP
  mysqlapp:
    port: 8090

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths: []
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

volume:
  baseDir: /opt/demo_app

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

configMap: {}

localdev:
  enabled: false

# Dependencies
mysql:
  fullnameOverride: "mysql"
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      # monitoring or prometheus in dev cluster?
      namespace: mysql-app
      interval: 30s
      scrapeTimeout: 15s

prometheus-mysql-exporter:
  fullnameOverride: "exporter"
  metrics:
    serviceMonitor:
      enabled: true
      # monitoring or prometheus in dev cluster?
      namespace: mysql-app
      interval: 30s
      scrapeTimeout: 15s

kube-prometheus-stack:
  fullnameOverride: "prometheus"
  nameOverride: "prometheus"
  alertmanager:
    enabled: false
  grafana:
    enabled: false
  kubeApiServer:
    enabled: true
  kubelet:
    enabled: false
  kubeControllerManager:
    enabled: false
  coreDns:
    enabled: false
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeProxy:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeStateMetrics:
    enabled: false
  nodeExporter:
    enabled: false
  prometheusOperator: 
    enabled: true
    resources: {}
  prometheus:
    ingress:
      enabled: false
    prometheusSpec:
      serviceMonitorSelectorNilUsesHelmValues: false
      serviceMonitorSelector: {}
  # kubelet:
  #   serviceMonitor:
  #     # When running on kind, cAdvisor will discover metrics via both the docker container that kind's node run in
  #     # and through Kubernetes. The result is that container metrics will be duplicated which causes problems.
  #     # To avoid that, drop the metrics sourced from kind's docker container.
  #     #
  #     # For more, see: https://app.clubhouse.io/asserts/story/2648/
  #     cAdvisorMetricRelabelings:
  #      - sourceLabels: [id]
  #        regex: \/docker\/.*
  #        action: drop